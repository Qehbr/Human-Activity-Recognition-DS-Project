{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:29:37.776662400Z",
     "start_time": "2024-02-20T17:29:34.945661Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from LSTM.lstm_autoencoder import LSTM_AE\n",
    "from LSTM.lstm_autoencoders_utils import train_lstm_autoencoder\n",
    "from NN.NeuralNetwork import NeuralNetwork\n",
    "from NN.nn_utils import get_train_data, train_nn_model\n",
    "from models_utils.Datasets import DataframeWithLabels, pad_sequence\n",
    "from models_utils.GLOBALS import *\n",
    "from models_utils.utils import convert_to_features"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get train data\n",
    "train_data = pd.read_csv('csv/train.csv')\n",
    "data_type_1 = train_data[train_data['sensor'] == 'vicon']['id'].tolist()\n",
    "data_type_2 = train_data[train_data['sensor'] == 'smartwatch']['id'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:29:37.810111300Z",
     "start_time": "2024-02-20T17:29:37.778162900Z"
    }
   },
   "id": "d8f0cf28027bd353",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# sizes of padding/cutting\n",
    "embedding_size = 128\n",
    "target_size_type1 = 4000\n",
    "target_size_type2 = 1350"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T17:29:37.818220200Z",
     "start_time": "2024-02-20T17:29:37.811111200Z"
    }
   },
   "id": "8e8da949e16e05f3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 20/176, Train Loss: 0.1344\n",
      "Batch: 40/176, Train Loss: 0.1259\n",
      "Batch: 60/176, Train Loss: 0.1319\n",
      "Batch: 80/176, Train Loss: 0.1295\n",
      "Batch: 100/176, Train Loss: 0.1272\n",
      "Batch: 120/176, Train Loss: 0.1209\n",
      "Batch: 140/176, Train Loss: 0.1208\n",
      "Batch: 160/176, Train Loss: 0.1166\n",
      "Batch: 176/176, Train Loss: 0.1189\n",
      "Epoch [1/15], Average Training Loss: 0.1262, , Average Validation Loss: 0.1179\n",
      "Batch: 20/176, Train Loss: 0.1188\n",
      "Batch: 40/176, Train Loss: 0.1133\n",
      "Batch: 60/176, Train Loss: 0.1111\n",
      "Batch: 80/176, Train Loss: 0.1064\n",
      "Batch: 100/176, Train Loss: 0.1083\n",
      "Batch: 120/176, Train Loss: 0.1062\n",
      "Batch: 140/176, Train Loss: 0.1048\n",
      "Batch: 160/176, Train Loss: 0.0966\n",
      "Batch: 176/176, Train Loss: 0.1006\n",
      "Epoch [2/15], Average Training Loss: 0.1080, , Average Validation Loss: 0.0986\n",
      "Batch: 20/176, Train Loss: 0.0987\n",
      "Batch: 40/176, Train Loss: 0.0933\n",
      "Batch: 60/176, Train Loss: 0.0906\n",
      "Batch: 80/176, Train Loss: 0.0855\n",
      "Batch: 100/176, Train Loss: 0.0864\n",
      "Batch: 120/176, Train Loss: 0.0837\n",
      "Batch: 140/176, Train Loss: 0.0815\n",
      "Batch: 160/176, Train Loss: 0.0733\n",
      "Batch: 176/176, Train Loss: 0.0758\n",
      "Epoch [3/15], Average Training Loss: 0.0865, , Average Validation Loss: 0.0739\n",
      "Batch: 20/176, Train Loss: 0.0726\n",
      "Batch: 40/176, Train Loss: 0.0669\n",
      "Batch: 60/176, Train Loss: 0.0628\n",
      "Batch: 80/176, Train Loss: 0.0567\n",
      "Batch: 100/176, Train Loss: 0.0557\n",
      "Batch: 120/176, Train Loss: 0.0515\n",
      "Batch: 140/176, Train Loss: 0.0477\n",
      "Batch: 160/176, Train Loss: 0.0391\n",
      "Batch: 176/176, Train Loss: 0.0388\n",
      "Epoch [4/15], Average Training Loss: 0.0566, , Average Validation Loss: 0.0371\n",
      "Batch: 20/176, Train Loss: 0.0331\n",
      "Batch: 40/176, Train Loss: 0.0272\n",
      "Batch: 60/176, Train Loss: 0.0214\n",
      "Batch: 80/176, Train Loss: 0.0153\n",
      "Batch: 100/176, Train Loss: 0.0131\n",
      "Batch: 120/176, Train Loss: 0.0106\n",
      "Batch: 140/176, Train Loss: 0.0098\n",
      "Batch: 160/176, Train Loss: 0.0080\n",
      "Batch: 176/176, Train Loss: 0.0089\n",
      "Epoch [5/15], Average Training Loss: 0.0180, , Average Validation Loss: 0.0081\n",
      "Batch: 20/176, Train Loss: 0.0072\n",
      "Batch: 40/176, Train Loss: 0.0082\n",
      "Batch: 60/176, Train Loss: 0.0072\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0072\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [6/15], Average Training Loss: 0.0078, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [7/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [8/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [9/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [10/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [11/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0071\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [12/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0078\n",
      "Batch: 20/176, Train Loss: 0.0070\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [13/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0077\n",
      "Batch: 20/176, Train Loss: 0.0070\n",
      "Batch: 40/176, Train Loss: 0.0079\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [14/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0077\n",
      "Batch: 20/176, Train Loss: 0.0070\n",
      "Batch: 40/176, Train Loss: 0.0078\n",
      "Batch: 60/176, Train Loss: 0.0071\n",
      "Batch: 80/176, Train Loss: 0.0067\n",
      "Batch: 100/176, Train Loss: 0.0075\n",
      "Batch: 120/176, Train Loss: 0.0079\n",
      "Batch: 140/176, Train Loss: 0.0085\n",
      "Batch: 160/176, Train Loss: 0.0071\n",
      "Batch: 176/176, Train Loss: 0.0085\n",
      "Epoch [15/15], Average Training Loss: 0.0077, , Average Validation Loss: 0.0077\n",
      "Batch: 20/453, Train Loss: 0.2309\n",
      "Batch: 40/453, Train Loss: 0.2308\n",
      "Batch: 60/453, Train Loss: 0.2385\n",
      "Batch: 80/453, Train Loss: 0.2246\n",
      "Batch: 100/453, Train Loss: 0.2193\n",
      "Batch: 120/453, Train Loss: 0.2327\n",
      "Batch: 140/453, Train Loss: 0.2246\n",
      "Batch: 160/453, Train Loss: 0.2248\n",
      "Batch: 180/453, Train Loss: 0.2158\n",
      "Batch: 200/453, Train Loss: 0.2066\n",
      "Batch: 220/453, Train Loss: 0.2083\n",
      "Batch: 240/453, Train Loss: 0.2027\n",
      "Batch: 260/453, Train Loss: 0.2166\n",
      "Batch: 280/453, Train Loss: 0.1923\n",
      "Batch: 300/453, Train Loss: 0.1869\n",
      "Batch: 320/453, Train Loss: 0.1927\n",
      "Batch: 340/453, Train Loss: 0.1854\n",
      "Batch: 360/453, Train Loss: 0.1794\n",
      "Batch: 380/453, Train Loss: 0.1682\n",
      "Batch: 400/453, Train Loss: 0.1594\n",
      "Batch: 420/453, Train Loss: 0.1493\n",
      "Batch: 440/453, Train Loss: 0.1537\n",
      "Batch: 453/453, Train Loss: 0.1534\n",
      "Epoch [1/15], Average Training Loss: 0.2025, , Average Validation Loss: 0.1554\n",
      "Batch: 20/453, Train Loss: 0.1576\n",
      "Batch: 40/453, Train Loss: 0.1562\n",
      "Batch: 60/453, Train Loss: 0.1477\n",
      "Batch: 80/453, Train Loss: 0.1245\n",
      "Batch: 100/453, Train Loss: 0.1199\n",
      "Batch: 120/453, Train Loss: 0.1177\n",
      "Batch: 140/453, Train Loss: 0.1116\n",
      "Batch: 160/453, Train Loss: 0.0939\n",
      "Batch: 180/453, Train Loss: 0.0901\n",
      "Batch: 200/453, Train Loss: 0.0802\n",
      "Batch: 220/453, Train Loss: 0.0756\n",
      "Batch: 240/453, Train Loss: 0.0618\n",
      "Batch: 260/453, Train Loss: 0.0581\n",
      "Batch: 280/453, Train Loss: 0.0414\n",
      "Batch: 300/453, Train Loss: 0.0279\n",
      "Batch: 320/453, Train Loss: 0.0197\n",
      "Batch: 340/453, Train Loss: 0.0194\n",
      "Batch: 360/453, Train Loss: 0.0172\n",
      "Batch: 380/453, Train Loss: 0.0169\n",
      "Batch: 400/453, Train Loss: 0.0161\n",
      "Batch: 420/453, Train Loss: 0.0159\n",
      "Batch: 440/453, Train Loss: 0.0172\n",
      "Batch: 453/453, Train Loss: 0.0167\n",
      "Epoch [2/15], Average Training Loss: 0.0725, , Average Validation Loss: 0.0162\n",
      "Batch: 20/453, Train Loss: 0.0170\n",
      "Batch: 40/453, Train Loss: 0.0168\n",
      "Batch: 60/453, Train Loss: 0.0184\n",
      "Batch: 80/453, Train Loss: 0.0156\n",
      "Batch: 100/453, Train Loss: 0.0169\n",
      "Batch: 120/453, Train Loss: 0.0147\n",
      "Batch: 140/453, Train Loss: 0.0160\n",
      "Batch: 160/453, Train Loss: 0.0164\n",
      "Batch: 180/453, Train Loss: 0.0147\n",
      "Batch: 200/453, Train Loss: 0.0173\n",
      "Batch: 220/453, Train Loss: 0.0161\n",
      "Batch: 240/453, Train Loss: 0.0155\n",
      "Batch: 260/453, Train Loss: 0.0169\n",
      "Batch: 280/453, Train Loss: 0.0163\n",
      "Batch: 300/453, Train Loss: 0.0148\n",
      "Batch: 320/453, Train Loss: 0.0146\n",
      "Batch: 340/453, Train Loss: 0.0175\n",
      "Batch: 360/453, Train Loss: 0.0163\n",
      "Batch: 380/453, Train Loss: 0.0163\n",
      "Batch: 400/453, Train Loss: 0.0154\n",
      "Batch: 420/453, Train Loss: 0.0153\n",
      "Batch: 440/453, Train Loss: 0.0166\n",
      "Batch: 453/453, Train Loss: 0.0158\n",
      "Epoch [3/15], Average Training Loss: 0.0160, , Average Validation Loss: 0.0157\n",
      "Batch: 20/453, Train Loss: 0.0165\n",
      "Batch: 40/453, Train Loss: 0.0164\n",
      "Batch: 60/453, Train Loss: 0.0179\n",
      "Batch: 80/453, Train Loss: 0.0152\n",
      "Batch: 100/453, Train Loss: 0.0163\n",
      "Batch: 120/453, Train Loss: 0.0142\n",
      "Batch: 140/453, Train Loss: 0.0155\n",
      "Batch: 160/453, Train Loss: 0.0159\n",
      "Batch: 180/453, Train Loss: 0.0143\n",
      "Batch: 200/453, Train Loss: 0.0167\n",
      "Batch: 220/453, Train Loss: 0.0156\n",
      "Batch: 240/453, Train Loss: 0.0150\n",
      "Batch: 260/453, Train Loss: 0.0163\n",
      "Batch: 280/453, Train Loss: 0.0158\n",
      "Batch: 300/453, Train Loss: 0.0144\n",
      "Batch: 320/453, Train Loss: 0.0141\n",
      "Batch: 340/453, Train Loss: 0.0171\n",
      "Batch: 360/453, Train Loss: 0.0158\n",
      "Batch: 380/453, Train Loss: 0.0157\n",
      "Batch: 400/453, Train Loss: 0.0148\n",
      "Batch: 420/453, Train Loss: 0.0148\n",
      "Batch: 440/453, Train Loss: 0.0160\n",
      "Batch: 453/453, Train Loss: 0.0152\n",
      "Epoch [4/15], Average Training Loss: 0.0155, , Average Validation Loss: 0.0151\n",
      "Batch: 20/453, Train Loss: 0.0159\n",
      "Batch: 40/453, Train Loss: 0.0158\n",
      "Batch: 60/453, Train Loss: 0.0174\n",
      "Batch: 80/453, Train Loss: 0.0148\n",
      "Batch: 100/453, Train Loss: 0.0157\n",
      "Batch: 120/453, Train Loss: 0.0137\n",
      "Batch: 140/453, Train Loss: 0.0150\n",
      "Batch: 160/453, Train Loss: 0.0153\n",
      "Batch: 180/453, Train Loss: 0.0137\n",
      "Batch: 200/453, Train Loss: 0.0160\n",
      "Batch: 220/453, Train Loss: 0.0150\n",
      "Batch: 240/453, Train Loss: 0.0143\n",
      "Batch: 260/453, Train Loss: 0.0156\n",
      "Batch: 280/453, Train Loss: 0.0152\n",
      "Batch: 300/453, Train Loss: 0.0139\n",
      "Batch: 320/453, Train Loss: 0.0136\n",
      "Batch: 340/453, Train Loss: 0.0166\n",
      "Batch: 360/453, Train Loss: 0.0152\n",
      "Batch: 380/453, Train Loss: 0.0151\n",
      "Batch: 400/453, Train Loss: 0.0142\n",
      "Batch: 420/453, Train Loss: 0.0142\n",
      "Batch: 440/453, Train Loss: 0.0154\n",
      "Batch: 453/453, Train Loss: 0.0145\n",
      "Epoch [5/15], Average Training Loss: 0.0149, , Average Validation Loss: 0.0145\n",
      "Batch: 20/453, Train Loss: 0.0152\n",
      "Batch: 40/453, Train Loss: 0.0152\n",
      "Batch: 60/453, Train Loss: 0.0169\n",
      "Batch: 80/453, Train Loss: 0.0143\n",
      "Batch: 100/453, Train Loss: 0.0149\n",
      "Batch: 120/453, Train Loss: 0.0131\n",
      "Batch: 140/453, Train Loss: 0.0143\n",
      "Batch: 160/453, Train Loss: 0.0147\n",
      "Batch: 180/453, Train Loss: 0.0130\n",
      "Batch: 200/453, Train Loss: 0.0153\n",
      "Batch: 220/453, Train Loss: 0.0143\n",
      "Batch: 240/453, Train Loss: 0.0137\n",
      "Batch: 260/453, Train Loss: 0.0149\n",
      "Batch: 280/453, Train Loss: 0.0146\n",
      "Batch: 300/453, Train Loss: 0.0134\n",
      "Batch: 320/453, Train Loss: 0.0130\n",
      "Batch: 340/453, Train Loss: 0.0164\n",
      "Batch: 360/453, Train Loss: 0.0147\n",
      "Batch: 380/453, Train Loss: 0.0144\n",
      "Batch: 400/453, Train Loss: 0.0136\n",
      "Batch: 420/453, Train Loss: 0.0137\n",
      "Batch: 440/453, Train Loss: 0.0148\n",
      "Batch: 453/453, Train Loss: 0.0137\n",
      "Epoch [6/15], Average Training Loss: 0.0143, , Average Validation Loss: 0.0138\n",
      "Batch: 20/453, Train Loss: 0.0146\n",
      "Batch: 40/453, Train Loss: 0.0147\n",
      "Batch: 60/453, Train Loss: 0.0164\n",
      "Batch: 80/453, Train Loss: 0.0138\n",
      "Batch: 100/453, Train Loss: 0.0143\n",
      "Batch: 120/453, Train Loss: 0.0126\n",
      "Batch: 140/453, Train Loss: 0.0137\n",
      "Batch: 160/453, Train Loss: 0.0140\n",
      "Batch: 180/453, Train Loss: 0.0124\n",
      "Batch: 200/453, Train Loss: 0.0146\n",
      "Batch: 220/453, Train Loss: 0.0136\n",
      "Batch: 240/453, Train Loss: 0.0131\n",
      "Batch: 260/453, Train Loss: 0.0142\n",
      "Batch: 280/453, Train Loss: 0.0140\n",
      "Batch: 300/453, Train Loss: 0.0129\n",
      "Batch: 320/453, Train Loss: 0.0125\n",
      "Batch: 340/453, Train Loss: 0.0166\n",
      "Batch: 360/453, Train Loss: 0.0142\n",
      "Batch: 380/453, Train Loss: 0.0138\n",
      "Batch: 400/453, Train Loss: 0.0130\n",
      "Batch: 420/453, Train Loss: 0.0133\n",
      "Batch: 440/453, Train Loss: 0.0142\n",
      "Batch: 453/453, Train Loss: 0.0129\n",
      "Epoch [7/15], Average Training Loss: 0.0137, , Average Validation Loss: 0.0133\n",
      "Batch: 20/453, Train Loss: 0.0140\n",
      "Batch: 40/453, Train Loss: 0.0143\n",
      "Batch: 60/453, Train Loss: 0.0160\n",
      "Batch: 80/453, Train Loss: 0.0134\n",
      "Batch: 100/453, Train Loss: 0.0138\n",
      "Batch: 120/453, Train Loss: 0.0122\n",
      "Batch: 140/453, Train Loss: 0.0131\n",
      "Batch: 160/453, Train Loss: 0.0135\n",
      "Batch: 180/453, Train Loss: 0.0119\n",
      "Batch: 200/453, Train Loss: 0.0140\n",
      "Batch: 220/453, Train Loss: 0.0131\n",
      "Batch: 240/453, Train Loss: 0.0126\n",
      "Batch: 260/453, Train Loss: 0.0136\n",
      "Batch: 280/453, Train Loss: 0.0135\n",
      "Batch: 300/453, Train Loss: 0.0125\n",
      "Batch: 320/453, Train Loss: 0.0120\n",
      "Batch: 340/453, Train Loss: 0.0166\n",
      "Batch: 360/453, Train Loss: 0.0137\n",
      "Batch: 380/453, Train Loss: 0.0133\n",
      "Batch: 400/453, Train Loss: 0.0125\n",
      "Batch: 420/453, Train Loss: 0.0129\n",
      "Batch: 440/453, Train Loss: 0.0137\n",
      "Batch: 453/453, Train Loss: 0.0122\n",
      "Epoch [8/15], Average Training Loss: 0.0132, , Average Validation Loss: 0.0128\n",
      "Batch: 20/453, Train Loss: 0.0134\n",
      "Batch: 40/453, Train Loss: 0.0139\n",
      "Batch: 60/453, Train Loss: 0.0154\n",
      "Batch: 80/453, Train Loss: 0.0130\n",
      "Batch: 100/453, Train Loss: 0.0132\n",
      "Batch: 120/453, Train Loss: 0.0117\n",
      "Batch: 140/453, Train Loss: 0.0126\n",
      "Batch: 160/453, Train Loss: 0.0129\n",
      "Batch: 180/453, Train Loss: 0.0114\n",
      "Batch: 200/453, Train Loss: 0.0133\n",
      "Batch: 220/453, Train Loss: 0.0125\n",
      "Batch: 240/453, Train Loss: 0.0120\n",
      "Batch: 260/453, Train Loss: 0.0130\n",
      "Batch: 280/453, Train Loss: 0.0129\n",
      "Batch: 300/453, Train Loss: 0.0119\n",
      "Batch: 320/453, Train Loss: 0.0115\n",
      "Batch: 340/453, Train Loss: 0.0161\n",
      "Batch: 360/453, Train Loss: 0.0129\n",
      "Batch: 380/453, Train Loss: 0.0126\n",
      "Batch: 400/453, Train Loss: 0.0119\n",
      "Batch: 420/453, Train Loss: 0.0124\n",
      "Batch: 440/453, Train Loss: 0.0130\n",
      "Batch: 453/453, Train Loss: 0.0111\n",
      "Epoch [9/15], Average Training Loss: 0.0126, , Average Validation Loss: 0.0121\n",
      "Batch: 20/453, Train Loss: 0.0127\n",
      "Batch: 40/453, Train Loss: 0.0133\n",
      "Batch: 60/453, Train Loss: 0.0148\n",
      "Batch: 80/453, Train Loss: 0.0125\n",
      "Batch: 100/453, Train Loss: 0.0125\n",
      "Batch: 120/453, Train Loss: 0.0111\n",
      "Batch: 140/453, Train Loss: 0.0119\n",
      "Batch: 160/453, Train Loss: 0.0122\n",
      "Batch: 180/453, Train Loss: 0.0108\n",
      "Batch: 200/453, Train Loss: 0.0124\n",
      "Batch: 220/453, Train Loss: 0.0117\n",
      "Batch: 240/453, Train Loss: 0.0113\n",
      "Batch: 260/453, Train Loss: 0.0123\n",
      "Batch: 280/453, Train Loss: 0.0121\n",
      "Batch: 300/453, Train Loss: 0.0111\n",
      "Batch: 320/453, Train Loss: 0.0107\n",
      "Batch: 340/453, Train Loss: 0.0154\n",
      "Batch: 360/453, Train Loss: 0.0120\n",
      "Batch: 380/453, Train Loss: 0.0117\n",
      "Batch: 400/453, Train Loss: 0.0111\n",
      "Batch: 420/453, Train Loss: 0.0117\n",
      "Batch: 440/453, Train Loss: 0.0121\n",
      "Batch: 453/453, Train Loss: 0.0098\n",
      "Epoch [10/15], Average Training Loss: 0.0118, , Average Validation Loss: 0.0112\n",
      "Batch: 20/453, Train Loss: 0.0117\n",
      "Batch: 40/453, Train Loss: 0.0126\n",
      "Batch: 60/453, Train Loss: 0.0139\n",
      "Batch: 80/453, Train Loss: 0.0119\n",
      "Batch: 100/453, Train Loss: 0.0116\n",
      "Batch: 120/453, Train Loss: 0.0104\n",
      "Batch: 140/453, Train Loss: 0.0109\n",
      "Batch: 160/453, Train Loss: 0.0113\n",
      "Batch: 180/453, Train Loss: 0.0100\n",
      "Batch: 200/453, Train Loss: 0.0113\n",
      "Batch: 220/453, Train Loss: 0.0108\n",
      "Batch: 240/453, Train Loss: 0.0104\n",
      "Batch: 260/453, Train Loss: 0.0113\n",
      "Batch: 280/453, Train Loss: 0.0111\n",
      "Batch: 300/453, Train Loss: 0.0101\n",
      "Batch: 320/453, Train Loss: 0.0098\n",
      "Batch: 340/453, Train Loss: 0.0142\n",
      "Batch: 360/453, Train Loss: 0.0108\n",
      "Batch: 380/453, Train Loss: 0.0107\n",
      "Batch: 400/453, Train Loss: 0.0103\n",
      "Batch: 420/453, Train Loss: 0.0109\n",
      "Batch: 440/453, Train Loss: 0.0110\n",
      "Batch: 453/453, Train Loss: 0.0081\n",
      "Epoch [11/15], Average Training Loss: 0.0109, , Average Validation Loss: 0.0103\n",
      "Batch: 20/453, Train Loss: 0.0107\n",
      "Batch: 40/453, Train Loss: 0.0118\n",
      "Batch: 60/453, Train Loss: 0.0129\n",
      "Batch: 80/453, Train Loss: 0.0113\n",
      "Batch: 100/453, Train Loss: 0.0107\n",
      "Batch: 120/453, Train Loss: 0.0096\n",
      "Batch: 140/453, Train Loss: 0.0099\n",
      "Batch: 160/453, Train Loss: 0.0103\n",
      "Batch: 180/453, Train Loss: 0.0092\n",
      "Batch: 200/453, Train Loss: 0.0103\n",
      "Batch: 220/453, Train Loss: 0.0099\n",
      "Batch: 240/453, Train Loss: 0.0097\n",
      "Batch: 260/453, Train Loss: 0.0106\n",
      "Batch: 280/453, Train Loss: 0.0103\n",
      "Batch: 300/453, Train Loss: 0.0092\n",
      "Batch: 320/453, Train Loss: 0.0091\n",
      "Batch: 340/453, Train Loss: 0.0130\n",
      "Batch: 360/453, Train Loss: 0.0098\n",
      "Batch: 380/453, Train Loss: 0.0100\n",
      "Batch: 400/453, Train Loss: 0.0097\n",
      "Batch: 420/453, Train Loss: 0.0102\n",
      "Batch: 440/453, Train Loss: 0.0104\n",
      "Batch: 453/453, Train Loss: 0.0069\n",
      "Epoch [12/15], Average Training Loss: 0.0100, , Average Validation Loss: 0.0096\n",
      "Batch: 20/453, Train Loss: 0.0100\n",
      "Batch: 40/453, Train Loss: 0.0112\n",
      "Batch: 60/453, Train Loss: 0.0123\n",
      "Batch: 80/453, Train Loss: 0.0109\n",
      "Batch: 100/453, Train Loss: 0.0102\n",
      "Batch: 120/453, Train Loss: 0.0091\n",
      "Batch: 140/453, Train Loss: 0.0092\n",
      "Batch: 160/453, Train Loss: 0.0098\n",
      "Batch: 180/453, Train Loss: 0.0087\n",
      "Batch: 200/453, Train Loss: 0.0099\n",
      "Batch: 220/453, Train Loss: 0.0095\n",
      "Batch: 240/453, Train Loss: 0.0093\n",
      "Batch: 260/453, Train Loss: 0.0102\n",
      "Batch: 280/453, Train Loss: 0.0099\n",
      "Batch: 300/453, Train Loss: 0.0087\n",
      "Batch: 320/453, Train Loss: 0.0086\n",
      "Batch: 340/453, Train Loss: 0.0124\n",
      "Batch: 360/453, Train Loss: 0.0093\n",
      "Batch: 380/453, Train Loss: 0.0095\n",
      "Batch: 400/453, Train Loss: 0.0095\n",
      "Batch: 420/453, Train Loss: 0.0099\n",
      "Batch: 440/453, Train Loss: 0.0101\n",
      "Batch: 453/453, Train Loss: 0.0063\n",
      "Epoch [13/15], Average Training Loss: 0.0095, , Average Validation Loss: 0.0092\n",
      "Batch: 20/453, Train Loss: 0.0097\n",
      "Batch: 40/453, Train Loss: 0.0108\n",
      "Batch: 60/453, Train Loss: 0.0118\n",
      "Batch: 80/453, Train Loss: 0.0106\n",
      "Batch: 100/453, Train Loss: 0.0100\n",
      "Batch: 120/453, Train Loss: 0.0087\n",
      "Batch: 140/453, Train Loss: 0.0087\n",
      "Batch: 160/453, Train Loss: 0.0094\n",
      "Batch: 180/453, Train Loss: 0.0083\n",
      "Batch: 200/453, Train Loss: 0.0096\n",
      "Batch: 220/453, Train Loss: 0.0092\n",
      "Batch: 240/453, Train Loss: 0.0091\n",
      "Batch: 260/453, Train Loss: 0.0099\n",
      "Batch: 280/453, Train Loss: 0.0097\n",
      "Batch: 300/453, Train Loss: 0.0083\n",
      "Batch: 320/453, Train Loss: 0.0083\n",
      "Batch: 340/453, Train Loss: 0.0121\n",
      "Batch: 360/453, Train Loss: 0.0090\n",
      "Batch: 380/453, Train Loss: 0.0092\n",
      "Batch: 400/453, Train Loss: 0.0094\n",
      "Batch: 420/453, Train Loss: 0.0097\n",
      "Batch: 440/453, Train Loss: 0.0099\n",
      "Batch: 453/453, Train Loss: 0.0059\n",
      "Epoch [14/15], Average Training Loss: 0.0092, , Average Validation Loss: 0.0089\n",
      "Batch: 20/453, Train Loss: 0.0095\n",
      "Batch: 40/453, Train Loss: 0.0104\n",
      "Batch: 60/453, Train Loss: 0.0114\n",
      "Batch: 80/453, Train Loss: 0.0103\n",
      "Batch: 100/453, Train Loss: 0.0098\n",
      "Batch: 120/453, Train Loss: 0.0083\n",
      "Batch: 140/453, Train Loss: 0.0083\n",
      "Batch: 160/453, Train Loss: 0.0090\n",
      "Batch: 180/453, Train Loss: 0.0080\n",
      "Batch: 200/453, Train Loss: 0.0094\n",
      "Batch: 220/453, Train Loss: 0.0090\n",
      "Batch: 240/453, Train Loss: 0.0088\n",
      "Batch: 260/453, Train Loss: 0.0096\n",
      "Batch: 280/453, Train Loss: 0.0094\n",
      "Batch: 300/453, Train Loss: 0.0081\n",
      "Batch: 320/453, Train Loss: 0.0080\n",
      "Batch: 340/453, Train Loss: 0.0118\n",
      "Batch: 360/453, Train Loss: 0.0087\n",
      "Batch: 380/453, Train Loss: 0.0089\n",
      "Batch: 400/453, Train Loss: 0.0093\n",
      "Batch: 420/453, Train Loss: 0.0095\n",
      "Batch: 440/453, Train Loss: 0.0098\n",
      "Batch: 453/453, Train Loss: 0.0055\n",
      "Epoch [15/15], Average Training Loss: 0.0089, , Average Validation Loss: 0.0087\n"
     ]
    }
   ],
   "source": [
    "# train or load models\n",
    "train_or_load_autoencoders = 'train'\n",
    "if train_or_load_autoencoders == 'train':\n",
    "    Type1LSTMAutoencoder = train_lstm_autoencoder(data_type_1, '1', target_size_type1, embedding_size, 0.000004, 64, 15)\n",
    "    Type2LSTMAutoencoder = train_lstm_autoencoder(data_type_2, '2', target_size_type2, embedding_size, 0.000004, 64, 15)\n",
    "elif train_or_load_autoencoders == 'load':\n",
    "    Type1LSTMAutoencoder = LSTM_AE(target_size_type1, 3, embedding_size).to(device)\n",
    "    Type1LSTMAutoencoder.load_state_dict(torch.load('Type1LSTMAutoencoder.pth'))\n",
    "    Type2LSTMAutoencoder = LSTM_AE(target_size_type2, 3, embedding_size).to(device)\n",
    "    Type2LSTMAutoencoder.load_state_dict(torch.load('Type2LSTMAutoencoder.pth'))\n",
    "else:\n",
    "    raise ValueError('Wrong train or load')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T18:14:52.272849400Z",
     "start_time": "2024-02-20T17:29:37.813720Z"
    }
   },
   "id": "e4b756be121ad35e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate or load train_data\n",
    "calculate_or_load_train_data = 'load'\n",
    "if calculate_or_load_train_data == 'calculate':\n",
    "    data_type_1, data_type_2 = get_train_data(Type1LSTMAutoencoder, Type2LSTMAutoencoder, embedding_size)\n",
    "elif calculate_or_load_train_data == 'load':\n",
    "    data_type_1 = pd.read_csv('train_data_type1.csv')\n",
    "    data_type_2 = pd.read_csv('train_data_type2.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T19:45:09.103744200Z",
     "start_time": "2024-02-16T19:45:08.407573Z"
    }
   },
   "id": "42b9423c5bece4e",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "NEURAL NETWORK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7760f2d0c09b75"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedding_names = [f'embedding_feature_{i + 1}' for i in range(embedding_size)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T19:45:09.108456500Z",
     "start_time": "2024-02-16T19:45:09.105391400Z"
    }
   },
   "id": "f16b0a69d354161",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# normalize data\n",
    "features_scaler_1 = StandardScaler()\n",
    "data_type_1_normalized = data_type_1\n",
    "columns_to_scale = data_type_1_normalized.columns.tolist()[embedding_size + 1:-1]\n",
    "data_type_1_normalized[columns_to_scale] = features_scaler_1.fit_transform(data_type_1_normalized[columns_to_scale])\n",
    "data_type_1_normalized['activity'] = data_type_1_normalized['activity'].map(activity_id_mapping)\n",
    "type1_dataset = DataframeWithLabels(data_type_1_normalized)\n",
    "\n",
    "features_scaler_2 = StandardScaler()\n",
    "data_type_2_normalized = data_type_2\n",
    "columns_to_scale = data_type_2_normalized.columns.tolist()[embedding_size + 1:-1]\n",
    "data_type_2_normalized[columns_to_scale] = features_scaler_2.fit_transform(data_type_2_normalized[columns_to_scale])\n",
    "data_type_2_normalized['activity'] = data_type_2_normalized['activity'].map(activity_id_mapping)\n",
    "type2_dataset = DataframeWithLabels(data_type_2_normalized)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T19:45:09.135810900Z",
     "start_time": "2024-02-16T19:45:09.108456500Z"
    }
   },
   "id": "76622320c52682b1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Epoch [1/100], Training Loss: 2.8184, Training Accuracy: 17.77%\n",
      "Epoch [1/100], Validation Loss: 2.7621, Validation Accuracy: 22.61%\n",
      "--------------\n",
      "Epoch [2/100], Training Loss: 2.7382, Training Accuracy: 26.15%\n",
      "Epoch [2/100], Validation Loss: 2.7150, Validation Accuracy: 29.40%\n",
      "--------------\n",
      "Epoch [3/100], Training Loss: 2.6948, Training Accuracy: 31.01%\n",
      "Epoch [3/100], Validation Loss: 2.6806, Validation Accuracy: 31.75%\n",
      "--------------\n",
      "Epoch [4/100], Training Loss: 2.6678, Training Accuracy: 32.65%\n",
      "Epoch [4/100], Validation Loss: 2.6617, Validation Accuracy: 33.20%\n",
      "--------------\n",
      "Epoch [5/100], Training Loss: 2.6467, Training Accuracy: 36.36%\n",
      "Epoch [5/100], Validation Loss: 2.6398, Validation Accuracy: 36.83%\n",
      "--------------\n",
      "Epoch [6/100], Training Loss: 2.6263, Training Accuracy: 38.66%\n",
      "Epoch [6/100], Validation Loss: 2.6203, Validation Accuracy: 40.10%\n",
      "--------------\n",
      "Epoch [7/100], Training Loss: 2.6086, Training Accuracy: 41.39%\n",
      "Epoch [7/100], Validation Loss: 2.6046, Validation Accuracy: 42.05%\n",
      "--------------\n",
      "Epoch [8/100], Training Loss: 2.5942, Training Accuracy: 43.05%\n",
      "Epoch [8/100], Validation Loss: 2.5926, Validation Accuracy: 43.33%\n",
      "--------------\n",
      "Epoch [9/100], Training Loss: 2.5827, Training Accuracy: 43.67%\n",
      "Epoch [9/100], Validation Loss: 2.5809, Validation Accuracy: 44.26%\n",
      "--------------\n",
      "Epoch [10/100], Training Loss: 2.5732, Training Accuracy: 44.51%\n",
      "Epoch [10/100], Validation Loss: 2.5732, Validation Accuracy: 44.40%\n",
      "--------------\n",
      "Epoch [11/100], Training Loss: 2.5654, Training Accuracy: 44.94%\n",
      "Epoch [11/100], Validation Loss: 2.5660, Validation Accuracy: 45.25%\n",
      "--------------\n",
      "Epoch [12/100], Training Loss: 2.5582, Training Accuracy: 45.63%\n",
      "Epoch [12/100], Validation Loss: 2.5609, Validation Accuracy: 44.76%\n",
      "--------------\n",
      "Epoch [13/100], Training Loss: 2.5517, Training Accuracy: 46.00%\n",
      "Epoch [13/100], Validation Loss: 2.5554, Validation Accuracy: 45.61%\n",
      "--------------\n",
      "Epoch [14/100], Training Loss: 2.5458, Training Accuracy: 46.56%\n",
      "Epoch [14/100], Validation Loss: 2.5463, Validation Accuracy: 46.78%\n",
      "--------------\n",
      "Epoch [15/100], Training Loss: 2.5380, Training Accuracy: 47.55%\n",
      "Epoch [15/100], Validation Loss: 2.5398, Validation Accuracy: 46.85%\n",
      "--------------\n",
      "Epoch [16/100], Training Loss: 2.5318, Training Accuracy: 47.97%\n",
      "Epoch [16/100], Validation Loss: 2.5365, Validation Accuracy: 47.10%\n",
      "--------------\n",
      "Epoch [17/100], Training Loss: 2.5259, Training Accuracy: 48.44%\n",
      "Epoch [17/100], Validation Loss: 2.5310, Validation Accuracy: 48.20%\n",
      "--------------\n",
      "Epoch [18/100], Training Loss: 2.5206, Training Accuracy: 49.03%\n",
      "Epoch [18/100], Validation Loss: 2.5254, Validation Accuracy: 48.52%\n",
      "--------------\n",
      "Epoch [19/100], Training Loss: 2.5162, Training Accuracy: 49.36%\n",
      "Epoch [19/100], Validation Loss: 2.5223, Validation Accuracy: 48.99%\n",
      "--------------\n",
      "Epoch [20/100], Training Loss: 2.5114, Training Accuracy: 50.00%\n",
      "Epoch [20/100], Validation Loss: 2.5193, Validation Accuracy: 48.92%\n",
      "--------------\n",
      "Epoch [21/100], Training Loss: 2.5075, Training Accuracy: 50.37%\n",
      "Epoch [21/100], Validation Loss: 2.5158, Validation Accuracy: 49.38%\n",
      "--------------\n",
      "Epoch [22/100], Training Loss: 2.5034, Training Accuracy: 50.55%\n",
      "Epoch [22/100], Validation Loss: 2.5138, Validation Accuracy: 49.24%\n",
      "--------------\n",
      "Epoch [23/100], Training Loss: 2.4998, Training Accuracy: 50.96%\n",
      "Epoch [23/100], Validation Loss: 2.5100, Validation Accuracy: 49.45%\n",
      "--------------\n",
      "Epoch [24/100], Training Loss: 2.4965, Training Accuracy: 51.41%\n",
      "Epoch [24/100], Validation Loss: 2.5074, Validation Accuracy: 49.63%\n",
      "--------------\n",
      "Epoch [25/100], Training Loss: 2.4927, Training Accuracy: 51.69%\n",
      "Epoch [25/100], Validation Loss: 2.5054, Validation Accuracy: 50.16%\n",
      "--------------\n",
      "Epoch [26/100], Training Loss: 2.4891, Training Accuracy: 52.15%\n",
      "Epoch [26/100], Validation Loss: 2.5019, Validation Accuracy: 50.37%\n",
      "--------------\n",
      "Epoch [27/100], Training Loss: 2.4860, Training Accuracy: 52.48%\n",
      "Epoch [27/100], Validation Loss: 2.4984, Validation Accuracy: 51.08%\n",
      "--------------\n",
      "Epoch [28/100], Training Loss: 2.4829, Training Accuracy: 52.77%\n",
      "Epoch [28/100], Validation Loss: 2.4974, Validation Accuracy: 50.91%\n",
      "--------------\n",
      "Epoch [29/100], Training Loss: 2.4801, Training Accuracy: 53.03%\n",
      "Epoch [29/100], Validation Loss: 2.4955, Validation Accuracy: 50.48%\n",
      "--------------\n",
      "Epoch [30/100], Training Loss: 2.4775, Training Accuracy: 53.13%\n",
      "Epoch [30/100], Validation Loss: 2.4934, Validation Accuracy: 51.62%\n",
      "--------------\n",
      "Epoch [31/100], Training Loss: 2.4752, Training Accuracy: 53.39%\n",
      "Epoch [31/100], Validation Loss: 2.4910, Validation Accuracy: 51.37%\n",
      "--------------\n",
      "Epoch [32/100], Training Loss: 2.4730, Training Accuracy: 53.60%\n",
      "Epoch [32/100], Validation Loss: 2.4891, Validation Accuracy: 51.87%\n",
      "--------------\n",
      "Epoch [33/100], Training Loss: 2.4700, Training Accuracy: 53.92%\n",
      "Epoch [33/100], Validation Loss: 2.4872, Validation Accuracy: 52.22%\n",
      "--------------\n",
      "Epoch [34/100], Training Loss: 2.4680, Training Accuracy: 53.90%\n",
      "Epoch [34/100], Validation Loss: 2.4851, Validation Accuracy: 52.08%\n",
      "--------------\n",
      "Epoch [35/100], Training Loss: 2.4654, Training Accuracy: 54.40%\n",
      "Epoch [35/100], Validation Loss: 2.4846, Validation Accuracy: 51.94%\n",
      "--------------\n",
      "Epoch [36/100], Training Loss: 2.4634, Training Accuracy: 54.43%\n",
      "Epoch [36/100], Validation Loss: 2.4824, Validation Accuracy: 52.26%\n",
      "--------------\n",
      "Epoch [37/100], Training Loss: 2.4619, Training Accuracy: 54.67%\n",
      "Epoch [37/100], Validation Loss: 2.4814, Validation Accuracy: 52.04%\n",
      "--------------\n",
      "Epoch [38/100], Training Loss: 2.4596, Training Accuracy: 54.72%\n",
      "Epoch [38/100], Validation Loss: 2.4803, Validation Accuracy: 52.33%\n",
      "--------------\n",
      "Epoch [39/100], Training Loss: 2.4572, Training Accuracy: 54.95%\n",
      "Epoch [39/100], Validation Loss: 2.4793, Validation Accuracy: 52.79%\n",
      "--------------\n",
      "Epoch [40/100], Training Loss: 2.4562, Training Accuracy: 54.78%\n",
      "Epoch [40/100], Validation Loss: 2.4773, Validation Accuracy: 52.51%\n",
      "--------------\n",
      "Epoch [41/100], Training Loss: 2.4544, Training Accuracy: 55.20%\n",
      "Epoch [41/100], Validation Loss: 2.4756, Validation Accuracy: 52.76%\n",
      "--------------\n",
      "Epoch [42/100], Training Loss: 2.4527, Training Accuracy: 55.56%\n",
      "Epoch [42/100], Validation Loss: 2.4751, Validation Accuracy: 52.58%\n",
      "--------------\n",
      "Epoch [43/100], Training Loss: 2.4503, Training Accuracy: 55.67%\n",
      "Epoch [43/100], Validation Loss: 2.4728, Validation Accuracy: 52.79%\n",
      "--------------\n",
      "Epoch [44/100], Training Loss: 2.4493, Training Accuracy: 55.51%\n",
      "Epoch [44/100], Validation Loss: 2.4716, Validation Accuracy: 53.00%\n",
      "--------------\n",
      "Epoch [45/100], Training Loss: 2.4478, Training Accuracy: 55.67%\n",
      "Epoch [45/100], Validation Loss: 2.4710, Validation Accuracy: 52.86%\n",
      "--------------\n",
      "Epoch [46/100], Training Loss: 2.4463, Training Accuracy: 55.86%\n",
      "Epoch [46/100], Validation Loss: 2.4706, Validation Accuracy: 52.68%\n",
      "--------------\n",
      "Epoch [47/100], Training Loss: 2.4444, Training Accuracy: 56.17%\n",
      "Epoch [47/100], Validation Loss: 2.4701, Validation Accuracy: 53.18%\n",
      "--------------\n",
      "Epoch [48/100], Training Loss: 2.4431, Training Accuracy: 56.20%\n",
      "Epoch [48/100], Validation Loss: 2.4699, Validation Accuracy: 53.15%\n",
      "--------------\n",
      "Epoch [49/100], Training Loss: 2.4412, Training Accuracy: 56.21%\n",
      "Epoch [49/100], Validation Loss: 2.4676, Validation Accuracy: 53.08%\n",
      "--------------\n",
      "Epoch [50/100], Training Loss: 2.4401, Training Accuracy: 56.41%\n",
      "Epoch [50/100], Validation Loss: 2.4667, Validation Accuracy: 53.36%\n",
      "--------------\n",
      "Epoch [51/100], Training Loss: 2.4384, Training Accuracy: 56.46%\n",
      "Epoch [51/100], Validation Loss: 2.4655, Validation Accuracy: 53.11%\n",
      "--------------\n",
      "Epoch [52/100], Training Loss: 2.4373, Training Accuracy: 56.81%\n",
      "Epoch [52/100], Validation Loss: 2.4629, Validation Accuracy: 53.50%\n",
      "--------------\n",
      "Epoch [53/100], Training Loss: 2.4353, Training Accuracy: 56.99%\n",
      "Epoch [53/100], Validation Loss: 2.4620, Validation Accuracy: 53.47%\n",
      "--------------\n",
      "Epoch [54/100], Training Loss: 2.4333, Training Accuracy: 57.14%\n",
      "Epoch [54/100], Validation Loss: 2.4629, Validation Accuracy: 53.15%\n",
      "--------------\n",
      "Epoch [55/100], Training Loss: 2.4320, Training Accuracy: 57.49%\n",
      "Epoch [55/100], Validation Loss: 2.4607, Validation Accuracy: 53.32%\n",
      "--------------\n",
      "Epoch [56/100], Training Loss: 2.4313, Training Accuracy: 57.36%\n",
      "Epoch [56/100], Validation Loss: 2.4598, Validation Accuracy: 53.71%\n",
      "--------------\n",
      "Epoch [57/100], Training Loss: 2.4290, Training Accuracy: 57.47%\n",
      "Epoch [57/100], Validation Loss: 2.4581, Validation Accuracy: 54.03%\n",
      "--------------\n",
      "Epoch [58/100], Training Loss: 2.4279, Training Accuracy: 57.67%\n",
      "Epoch [58/100], Validation Loss: 2.4607, Validation Accuracy: 53.89%\n",
      "--------------\n",
      "Epoch [59/100], Training Loss: 2.4267, Training Accuracy: 57.61%\n",
      "Epoch [59/100], Validation Loss: 2.4567, Validation Accuracy: 54.14%\n",
      "--------------\n",
      "Epoch [60/100], Training Loss: 2.4246, Training Accuracy: 58.05%\n",
      "Epoch [60/100], Validation Loss: 2.4554, Validation Accuracy: 54.35%\n",
      "--------------\n",
      "Epoch [61/100], Training Loss: 2.4231, Training Accuracy: 58.21%\n",
      "Epoch [61/100], Validation Loss: 2.4531, Validation Accuracy: 54.50%\n",
      "--------------\n",
      "Epoch [62/100], Training Loss: 2.4215, Training Accuracy: 58.29%\n",
      "Epoch [62/100], Validation Loss: 2.4514, Validation Accuracy: 54.46%\n",
      "--------------\n",
      "Epoch [63/100], Training Loss: 2.4194, Training Accuracy: 58.46%\n",
      "Epoch [63/100], Validation Loss: 2.4501, Validation Accuracy: 54.78%\n",
      "--------------\n",
      "Epoch [64/100], Training Loss: 2.4182, Training Accuracy: 58.82%\n",
      "Epoch [64/100], Validation Loss: 2.4503, Validation Accuracy: 54.89%\n",
      "--------------\n",
      "Epoch [65/100], Training Loss: 2.4165, Training Accuracy: 58.91%\n",
      "Epoch [65/100], Validation Loss: 2.4485, Validation Accuracy: 55.03%\n",
      "--------------\n",
      "Epoch [66/100], Training Loss: 2.4159, Training Accuracy: 58.96%\n",
      "Epoch [66/100], Validation Loss: 2.4488, Validation Accuracy: 54.67%\n",
      "--------------\n",
      "Epoch [67/100], Training Loss: 2.4150, Training Accuracy: 59.01%\n",
      "Epoch [67/100], Validation Loss: 2.4468, Validation Accuracy: 55.24%\n",
      "--------------\n",
      "Epoch [68/100], Training Loss: 2.4134, Training Accuracy: 59.25%\n",
      "Epoch [68/100], Validation Loss: 2.4462, Validation Accuracy: 55.10%\n",
      "--------------\n",
      "Epoch [69/100], Training Loss: 2.4126, Training Accuracy: 59.18%\n",
      "Epoch [69/100], Validation Loss: 2.4466, Validation Accuracy: 55.39%\n",
      "--------------\n",
      "Epoch [70/100], Training Loss: 2.4111, Training Accuracy: 59.40%\n",
      "Epoch [70/100], Validation Loss: 2.4450, Validation Accuracy: 54.96%\n",
      "--------------\n",
      "Epoch [71/100], Training Loss: 2.4101, Training Accuracy: 59.64%\n",
      "Epoch [71/100], Validation Loss: 2.4446, Validation Accuracy: 55.39%\n",
      "--------------\n",
      "Epoch [72/100], Training Loss: 2.4092, Training Accuracy: 59.56%\n",
      "Epoch [72/100], Validation Loss: 2.4437, Validation Accuracy: 55.49%\n",
      "--------------\n",
      "Epoch [73/100], Training Loss: 2.4083, Training Accuracy: 59.77%\n",
      "Epoch [73/100], Validation Loss: 2.4442, Validation Accuracy: 55.46%\n",
      "--------------\n",
      "Epoch [74/100], Training Loss: 2.4072, Training Accuracy: 59.89%\n",
      "Epoch [74/100], Validation Loss: 2.4434, Validation Accuracy: 55.39%\n",
      "--------------\n",
      "Epoch [75/100], Training Loss: 2.4057, Training Accuracy: 60.00%\n",
      "Epoch [75/100], Validation Loss: 2.4428, Validation Accuracy: 55.46%\n",
      "--------------\n",
      "Epoch [76/100], Training Loss: 2.4051, Training Accuracy: 59.88%\n",
      "Epoch [76/100], Validation Loss: 2.4417, Validation Accuracy: 55.60%\n",
      "--------------\n",
      "Epoch [77/100], Training Loss: 2.4042, Training Accuracy: 60.14%\n",
      "Epoch [77/100], Validation Loss: 2.4397, Validation Accuracy: 56.03%\n",
      "--------------\n",
      "Epoch [78/100], Training Loss: 2.4030, Training Accuracy: 60.22%\n",
      "Epoch [78/100], Validation Loss: 2.4390, Validation Accuracy: 55.95%\n",
      "--------------\n",
      "Epoch [79/100], Training Loss: 2.4022, Training Accuracy: 60.31%\n",
      "Epoch [79/100], Validation Loss: 2.4389, Validation Accuracy: 56.20%\n",
      "--------------\n",
      "Epoch [80/100], Training Loss: 2.4014, Training Accuracy: 60.29%\n",
      "Epoch [80/100], Validation Loss: 2.4375, Validation Accuracy: 56.31%\n",
      "--------------\n",
      "Epoch [81/100], Training Loss: 2.4002, Training Accuracy: 60.46%\n",
      "Epoch [81/100], Validation Loss: 2.4371, Validation Accuracy: 56.38%\n",
      "--------------\n",
      "Epoch [82/100], Training Loss: 2.3992, Training Accuracy: 60.61%\n",
      "Epoch [82/100], Validation Loss: 2.4364, Validation Accuracy: 56.38%\n",
      "--------------\n",
      "Epoch [83/100], Training Loss: 2.3978, Training Accuracy: 60.55%\n",
      "Epoch [83/100], Validation Loss: 2.4370, Validation Accuracy: 56.31%\n",
      "--------------\n",
      "Epoch [84/100], Training Loss: 2.3972, Training Accuracy: 60.72%\n",
      "Epoch [84/100], Validation Loss: 2.4361, Validation Accuracy: 56.24%\n",
      "--------------\n",
      "Epoch [85/100], Training Loss: 2.3963, Training Accuracy: 60.70%\n",
      "Epoch [85/100], Validation Loss: 2.4346, Validation Accuracy: 56.56%\n",
      "--------------\n",
      "Epoch [86/100], Training Loss: 2.3953, Training Accuracy: 60.90%\n",
      "Epoch [86/100], Validation Loss: 2.4336, Validation Accuracy: 56.52%\n",
      "--------------\n",
      "Epoch [87/100], Training Loss: 2.3948, Training Accuracy: 60.76%\n",
      "Epoch [87/100], Validation Loss: 2.4327, Validation Accuracy: 56.49%\n",
      "--------------\n",
      "Epoch [88/100], Training Loss: 2.3941, Training Accuracy: 60.93%\n",
      "Epoch [88/100], Validation Loss: 2.4323, Validation Accuracy: 56.81%\n",
      "--------------\n",
      "Epoch [89/100], Training Loss: 2.3928, Training Accuracy: 60.98%\n",
      "Epoch [89/100], Validation Loss: 2.4324, Validation Accuracy: 56.67%\n",
      "--------------\n",
      "Epoch [90/100], Training Loss: 2.3921, Training Accuracy: 61.29%\n",
      "Epoch [90/100], Validation Loss: 2.4320, Validation Accuracy: 56.99%\n",
      "--------------\n",
      "Epoch [91/100], Training Loss: 2.3915, Training Accuracy: 61.19%\n",
      "Epoch [91/100], Validation Loss: 2.4302, Validation Accuracy: 56.74%\n",
      "--------------\n",
      "Epoch [92/100], Training Loss: 2.3901, Training Accuracy: 61.16%\n",
      "Epoch [92/100], Validation Loss: 2.4318, Validation Accuracy: 56.38%\n",
      "--------------\n",
      "Epoch [93/100], Training Loss: 2.3902, Training Accuracy: 61.35%\n",
      "Epoch [93/100], Validation Loss: 2.4287, Validation Accuracy: 57.27%\n",
      "--------------\n",
      "Epoch [94/100], Training Loss: 2.3891, Training Accuracy: 61.35%\n",
      "Epoch [94/100], Validation Loss: 2.4287, Validation Accuracy: 56.88%\n",
      "--------------\n",
      "Epoch [95/100], Training Loss: 2.3879, Training Accuracy: 61.50%\n",
      "Epoch [95/100], Validation Loss: 2.4272, Validation Accuracy: 57.02%\n",
      "--------------\n",
      "Epoch [96/100], Training Loss: 2.3873, Training Accuracy: 61.58%\n",
      "Epoch [96/100], Validation Loss: 2.4274, Validation Accuracy: 57.20%\n",
      "--------------\n",
      "Epoch [97/100], Training Loss: 2.3866, Training Accuracy: 61.57%\n",
      "Epoch [97/100], Validation Loss: 2.4261, Validation Accuracy: 57.59%\n",
      "--------------\n",
      "Epoch [98/100], Training Loss: 2.3861, Training Accuracy: 61.74%\n",
      "Epoch [98/100], Validation Loss: 2.4255, Validation Accuracy: 57.45%\n",
      "--------------\n",
      "Epoch [99/100], Training Loss: 2.3853, Training Accuracy: 61.65%\n",
      "Epoch [99/100], Validation Loss: 2.4266, Validation Accuracy: 56.88%\n",
      "--------------\n",
      "Epoch [100/100], Training Loss: 2.3843, Training Accuracy: 61.89%\n",
      "Epoch [100/100], Validation Loss: 2.4247, Validation Accuracy: 57.45%\n"
     ]
    }
   ],
   "source": [
    "# train or load nn\n",
    "train_or_load_nn_model_type_1 = 'train'\n",
    "train_or_load_nn_model_type_2 = 'load'\n",
    "\n",
    "if train_or_load_nn_model_type_1 == 'train':\n",
    "    model_type_1 = train_nn_model(type1_dataset, '1', 174, [75], 18,\n",
    "                                  batch_size=64, learning_rate=0.0005, num_epochs=100, scheduler_factor=0.9,\n",
    "                                  scheduler_patience=4)\n",
    "elif train_or_load_nn_model_type_1 == 'load':\n",
    "    model_type_1 = NeuralNetwork(174, [75], 18).to(device)\n",
    "    model_type_1.load_state_dict(torch.load('Type1NNModel.pth'))\n",
    "else:\n",
    "    raise ValueError('Wrong train or load')\n",
    "\n",
    "if train_or_load_nn_model_type_2 == 'train':\n",
    "    model_type_2 = train_nn_model(type2_dataset, '2', 174, [75], 18,\n",
    "                                  batch_size=64, learning_rate=0.0005, num_epochs=100, scheduler_factor=0.9,\n",
    "                                  scheduler_patience=4)\n",
    "elif train_or_load_nn_model_type_2 == 'load':\n",
    "    model_type_2 = NeuralNetwork(174, [75], 18).to(device)\n",
    "    model_type_2.load_state_dict(torch.load('Type2NNModel.pth'))\n",
    "else:\n",
    "    raise ValueError('Wrong train or load')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T20:11:00.211634900Z",
     "start_time": "2024-02-16T20:05:38.014794500Z"
    }
   },
   "id": "785e8415236c0c0a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save results\n",
    "results_list = []\n",
    "for i, file_id in enumerate(pd.read_csv('sample_submission.csv')['sample_id'].to_list()):\n",
    "\n",
    "    class_path = os.path.join(files_directory, f\"{file_id}.csv\")\n",
    "    new_data = pd.read_csv(class_path)\n",
    "\n",
    "    if new_data.shape[1] == 3:\n",
    "\n",
    "        data_x_tensor = torch.tensor(new_data[\"x [m]\"].values, dtype=torch.float32)\n",
    "        data_y_tensor = torch.tensor(new_data[\"y [m]\"].values, dtype=torch.float32)\n",
    "        data_z_tensor = torch.tensor(new_data[\"z [m]\"].values, dtype=torch.float32)\n",
    "        new_features = convert_to_features(data_x_tensor, data_y_tensor, data_z_tensor)\n",
    "\n",
    "        if len(new_data) < 4000:\n",
    "            new_data = pad_sequence(new_data, 4000)\n",
    "\n",
    "        new_data = torch.tensor(new_data.values, dtype=torch.float32).to(device)\n",
    "        new_data = new_data.view(1, new_data.shape[0], new_data.shape[1])\n",
    "        normalized_new_data = (new_data - min_values_type1) / (max_values_type1 - min_values_type1 + 1e-6)\n",
    "        new_data_encoded = Type1LSTMAutoencoder.encode(normalized_new_data)\n",
    "        encoded_features = new_data_encoded.squeeze().detach().cpu().numpy()\n",
    "\n",
    "        res = pd.DataFrame([encoded_features], columns=embedding_names)\n",
    "        for col, value in new_features.items():\n",
    "            res[col] = value\n",
    "\n",
    "        columns_to_scale = res.columns.tolist()[embedding_size + 1:]\n",
    "        res[columns_to_scale] = features_scaler_1.transform(res[columns_to_scale])\n",
    "        x = torch.tensor(res.values, dtype=torch.float32).to(device)\n",
    "        predictions = model_type_1(x)\n",
    "\n",
    "    else:\n",
    "        new_data = new_data[new_data.iloc[:, 0] == 'acceleration [m/s/s]'].iloc[:, 1:]\n",
    "\n",
    "        data_x_tensor = torch.tensor(new_data[\"x\"].values, dtype=torch.float32)\n",
    "        data_y_tensor = torch.tensor(new_data[\"y\"].values, dtype=torch.float32)\n",
    "        data_z_tensor = torch.tensor(new_data[\"z\"].values, dtype=torch.float32)\n",
    "        new_features = convert_to_features(data_x_tensor, data_y_tensor, data_z_tensor)\n",
    "\n",
    "        if len(new_data) < 1350:\n",
    "            new_data = pad_sequence(new_data, 1350)\n",
    "\n",
    "        new_data = torch.tensor(new_data.values, dtype=torch.float32).to(device)\n",
    "        new_data = new_data.view(1, new_data.shape[0], new_data.shape[1])\n",
    "        normalized_new_data = (new_data - min_values_type2) / (max_values_type2 - min_values_type2 + 1e-6)\n",
    "        new_data_encoded = Type2LSTMAutoencoder.encode(new_data)\n",
    "        encoded_features = new_data_encoded.squeeze().detach().cpu().numpy()\n",
    "\n",
    "        res = pd.DataFrame([encoded_features], columns=embedding_names)\n",
    "        for col, value in new_features.items():\n",
    "            res[col] = value\n",
    "\n",
    "        columns_to_scale = res.columns.tolist()[embedding_size + 1:]\n",
    "        res[columns_to_scale] = features_scaler_2.transform(res[columns_to_scale])\n",
    "        x = torch.tensor(res.values, dtype=torch.float32).to(device).to(device)\n",
    "        predictions = model_type_2(x)\n",
    "\n",
    "    res_dict = {activity: predictions.squeeze()[id].item() for id, activity in id_activity_mapping.items()}\n",
    "\n",
    "    result_dict = {label: res_dict.get(label, 0) for label in activity_id_mapping.keys()}\n",
    "    result_dict['sample_id'] = file_id\n",
    "    results_list.append(result_dict)\n",
    "results = pd.DataFrame(results_list, columns=['sample_id'] + list(activity_id_mapping.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T20:55:28.228690600Z",
     "start_time": "2024-02-16T20:24:41.758680900Z"
    }
   },
   "id": "42bc6eb6b9d2c2d6",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results.fillna(0).to_csv('results_nn2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T21:08:48.763084800Z",
     "start_time": "2024-02-16T21:08:47.865643600Z"
    }
   },
   "id": "b7cb28976c563220",
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
