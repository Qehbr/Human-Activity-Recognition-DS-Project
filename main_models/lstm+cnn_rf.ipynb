{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:38:01.028849Z",
     "start_time": "2024-02-17T12:38:00.769911400Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from models_utils.Datasets import *\n",
    "from models_utils.GLOBALS import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get train data\n",
    "train_data = pd.read_csv('train.csv', index_col=0)\n",
    "train_data['activity'] = train_data['activity'].map(activity_id_mapping)\n",
    "data_type_1 = train_data[train_data['sensor'] == 'vicon'].reset_index()\n",
    "data_type_2 = train_data[train_data['sensor'] == 'smartwatch'].reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:38:01.230702100Z",
     "start_time": "2024-02-17T12:38:01.192679900Z"
    }
   },
   "id": "cf84bab6261e1945",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CNN / LSTM feature extractor\n",
    "class MultivariateCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_channels, input_length, num_classes=18, lstm_hidden_size=256, lstm_layers=1):\n",
    "        super(MultivariateCNNLSTM, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.input_length = input_length\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        output_size_after_conv2 = input_length // 4\n",
    "        self.lstm_input_size = 128\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=lstm_hidden_size, num_layers=lstm_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # output size from LSTM to first fc layer\n",
    "        self.fc1_size = lstm_hidden_size * output_size_after_conv2\n",
    "        self.fc1 = nn.Linear(self.fc1_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # reshape x  for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        # flatten the output for fc layer\n",
    "        x = x.contiguous().view(-1, self.fc1_size)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:38:01.638587800Z",
     "start_time": "2024-02-17T12:38:01.637086900Z"
    }
   },
   "id": "597f74bc6f99b114",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model with loss: 1.9149110588160427\n",
      "--------------\n",
      "Epoch [1/30], Epoch Duration: 31.54 seconds\n",
      "Epoch [1/30], Training Loss: 2.2161, Training Accuracy: 16.98%\n",
      "Epoch [1/30], Validation Loss: 1.9149, Validation Accuracy: 19.55%\n",
      "Saving Best Model with loss: 1.709758387370543\n",
      "--------------\n",
      "Epoch [2/30], Epoch Duration: 32.59 seconds\n",
      "Epoch [2/30], Training Loss: 1.8019, Training Accuracy: 24.83%\n",
      "Epoch [2/30], Validation Loss: 1.7098, Validation Accuracy: 26.48%\n",
      "Saving Best Model with loss: 1.6206989532167262\n",
      "--------------\n",
      "Epoch [3/30], Epoch Duration: 32.39 seconds\n",
      "Epoch [3/30], Training Loss: 1.6337, Training Accuracy: 32.18%\n",
      "Epoch [3/30], Validation Loss: 1.6207, Validation Accuracy: 33.49%\n",
      "--------------\n",
      "Epoch [4/30], Epoch Duration: 31.88 seconds\n",
      "Epoch [4/30], Training Loss: 1.5354, Training Accuracy: 37.52%\n",
      "Epoch [4/30], Validation Loss: 1.6332, Validation Accuracy: 32.99%\n",
      "Saving Best Model with loss: 1.4482392858375202\n",
      "--------------\n",
      "Epoch [5/30], Epoch Duration: 30.79 seconds\n",
      "Epoch [5/30], Training Loss: 1.4529, Training Accuracy: 41.45%\n",
      "Epoch [5/30], Validation Loss: 1.4482, Validation Accuracy: 39.64%\n",
      "Saving Best Model with loss: 1.367643191055818\n",
      "--------------\n",
      "Epoch [6/30], Epoch Duration: 31.03 seconds\n",
      "Epoch [6/30], Training Loss: 1.2810, Training Accuracy: 50.44%\n",
      "Epoch [6/30], Validation Loss: 1.3676, Validation Accuracy: 44.97%\n",
      "Saving Best Model with loss: 1.2390973649241708\n",
      "--------------\n",
      "Epoch [7/30], Epoch Duration: 30.94 seconds\n",
      "Epoch [7/30], Training Loss: 1.1334, Training Accuracy: 57.02%\n",
      "Epoch [7/30], Validation Loss: 1.2391, Validation Accuracy: 52.65%\n",
      "Saving Best Model with loss: 1.1708595021204515\n",
      "--------------\n",
      "Epoch [8/30], Epoch Duration: 31.11 seconds\n",
      "Epoch [8/30], Training Loss: 0.9791, Training Accuracy: 63.07%\n",
      "Epoch [8/30], Validation Loss: 1.1709, Validation Accuracy: 56.74%\n",
      "Saving Best Model with loss: 1.0866280767050656\n",
      "--------------\n",
      "Epoch [9/30], Epoch Duration: 31.15 seconds\n",
      "Epoch [9/30], Training Loss: 0.8272, Training Accuracy: 69.40%\n",
      "Epoch [9/30], Validation Loss: 1.0866, Validation Accuracy: 61.82%\n",
      "Saving Best Model with loss: 0.9641657295552167\n",
      "--------------\n",
      "Epoch [10/30], Epoch Duration: 30.90 seconds\n",
      "Epoch [10/30], Training Loss: 0.6598, Training Accuracy: 75.68%\n",
      "Epoch [10/30], Validation Loss: 0.9642, Validation Accuracy: 70.81%\n",
      "--------------\n",
      "Epoch [11/30], Epoch Duration: 31.10 seconds\n",
      "Epoch [11/30], Training Loss: 0.5497, Training Accuracy: 80.10%\n",
      "Epoch [11/30], Validation Loss: 1.1998, Validation Accuracy: 67.40%\n",
      "--------------\n",
      "Epoch [12/30], Epoch Duration: 30.96 seconds\n",
      "Epoch [12/30], Training Loss: 0.5273, Training Accuracy: 81.03%\n",
      "Epoch [12/30], Validation Loss: 0.9679, Validation Accuracy: 72.56%\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "--------------\n",
      "Epoch [13/30], Epoch Duration: 31.66 seconds\n",
      "Epoch [13/30], Training Loss: 0.3942, Training Accuracy: 85.53%\n",
      "Epoch [13/30], Validation Loss: 1.0161, Validation Accuracy: 75.01%\n",
      "Saving Best Model with loss: 0.9175861918113448\n",
      "--------------\n",
      "Epoch [14/30], Epoch Duration: 32.24 seconds\n",
      "Epoch [14/30], Training Loss: 0.2475, Training Accuracy: 91.50%\n",
      "Epoch [14/30], Validation Loss: 0.9176, Validation Accuracy: 77.64%\n",
      "--------------\n",
      "Epoch [15/30], Epoch Duration: 32.33 seconds\n",
      "Epoch [15/30], Training Loss: 0.2156, Training Accuracy: 92.49%\n",
      "Epoch [15/30], Validation Loss: 0.9462, Validation Accuracy: 79.24%\n",
      "--------------\n",
      "Epoch [16/30], Epoch Duration: 31.95 seconds\n",
      "Epoch [16/30], Training Loss: 0.2008, Training Accuracy: 93.01%\n",
      "Epoch [16/30], Validation Loss: 0.9798, Validation Accuracy: 79.42%\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "--------------\n",
      "Epoch [17/30], Epoch Duration: 31.02 seconds\n",
      "Epoch [17/30], Training Loss: 0.1899, Training Accuracy: 93.01%\n",
      "Epoch [17/30], Validation Loss: 1.0048, Validation Accuracy: 79.35%\n",
      "--------------\n",
      "Epoch [18/30], Epoch Duration: 31.05 seconds\n",
      "Epoch [18/30], Training Loss: 0.1755, Training Accuracy: 93.80%\n",
      "Epoch [18/30], Validation Loss: 1.0039, Validation Accuracy: 79.81%\n",
      "--------------\n",
      "Epoch [19/30], Epoch Duration: 31.95 seconds\n",
      "Epoch [19/30], Training Loss: 0.1721, Training Accuracy: 93.97%\n",
      "Epoch [19/30], Validation Loss: 1.0097, Validation Accuracy: 80.06%\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "--------------\n",
      "Epoch [20/30], Epoch Duration: 32.05 seconds\n",
      "Epoch [20/30], Training Loss: 0.1703, Training Accuracy: 94.04%\n",
      "Epoch [20/30], Validation Loss: 1.0110, Validation Accuracy: 80.23%\n",
      "--------------\n",
      "Epoch [21/30], Epoch Duration: 31.44 seconds\n",
      "Epoch [21/30], Training Loss: 0.1685, Training Accuracy: 94.11%\n",
      "Epoch [21/30], Validation Loss: 1.0127, Validation Accuracy: 80.20%\n",
      "--------------\n",
      "Epoch [22/30], Epoch Duration: 31.69 seconds\n",
      "Epoch [22/30], Training Loss: 0.1682, Training Accuracy: 94.06%\n",
      "Epoch [22/30], Validation Loss: 1.0140, Validation Accuracy: 80.23%\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-07.\n",
      "--------------\n",
      "Epoch [23/30], Epoch Duration: 32.55 seconds\n",
      "Epoch [23/30], Training Loss: 0.1682, Training Accuracy: 94.09%\n",
      "Epoch [23/30], Validation Loss: 1.0149, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [24/30], Epoch Duration: 32.20 seconds\n",
      "Epoch [24/30], Training Loss: 0.1679, Training Accuracy: 94.08%\n",
      "Epoch [24/30], Validation Loss: 1.0150, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [25/30], Epoch Duration: 32.04 seconds\n",
      "Epoch [25/30], Training Loss: 0.1679, Training Accuracy: 94.08%\n",
      "Epoch [25/30], Validation Loss: 1.0151, Validation Accuracy: 80.16%\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-08.\n",
      "--------------\n",
      "Epoch [26/30], Epoch Duration: 32.02 seconds\n",
      "Epoch [26/30], Training Loss: 0.1679, Training Accuracy: 94.08%\n",
      "Epoch [26/30], Validation Loss: 1.0152, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [27/30], Epoch Duration: 31.85 seconds\n",
      "Epoch [27/30], Training Loss: 0.1680, Training Accuracy: 94.08%\n",
      "Epoch [27/30], Validation Loss: 1.0152, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [28/30], Epoch Duration: 31.29 seconds\n",
      "Epoch [28/30], Training Loss: 0.1679, Training Accuracy: 94.08%\n",
      "Epoch [28/30], Validation Loss: 1.0152, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [29/30], Epoch Duration: 31.35 seconds\n",
      "Epoch [29/30], Training Loss: 0.1679, Training Accuracy: 94.08%\n",
      "Epoch [29/30], Validation Loss: 1.0152, Validation Accuracy: 80.16%\n",
      "--------------\n",
      "Epoch [30/30], Epoch Duration: 31.36 seconds\n",
      "Epoch [30/30], Training Loss: 0.1677, Training Accuracy: 94.08%\n",
      "Epoch [30/30], Validation Loss: 1.0152, Validation Accuracy: 80.16%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Type1CNNLSTMModel.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 93\u001B[0m\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     90\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Validation Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_val_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Validation Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_accuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     92\u001B[0m model_CNN_LSTM_type1 \u001B[38;5;241m=\u001B[39m MultivariateCNNLSTM(\u001B[38;5;241m3\u001B[39m, target_size_type1, \u001B[38;5;241m18\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 93\u001B[0m model_CNN_LSTM_type1\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mType1CNNLSTMModel.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dlworkshop\\Lib\\site-packages\\torch\\serialization.py:986\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    984\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 986\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    987\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    989\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    991\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dlworkshop\\Lib\\site-packages\\torch\\serialization.py:435\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 435\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dlworkshop\\Lib\\site-packages\\torch\\serialization.py:416\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 416\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Type1CNNLSTMModel.pth'"
     ]
    }
   ],
   "source": [
    "# train first model\n",
    "target_size_type1 = 3000\n",
    "whole_dataset_type1 = TrainDataframeWithLabels(data_type_1, '1', target_size_type1)\n",
    "model_CNN_LSTM_type1 = MultivariateCNNLSTM(3, target_size_type1, 18).to(device)\n",
    "\n",
    "labels = data_type_1['activity'].to_list()\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(whole_dataset_type1)),\n",
    "    test_size=0.2, \n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "# create stratified datasets\n",
    "train_dataset = Subset(whole_dataset_type1, train_idx)\n",
    "val_dataset = Subset(whole_dataset_type1, val_idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN_LSTM_type1.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "best_val_loss = float('inf') \n",
    "best_model_state = copy.deepcopy(model_CNN_LSTM_type1.state_dict())  \n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    total_val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # training \n",
    "    model_CNN_LSTM_type1.train() \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        normalized_inputs = (inputs - min_values_type1) / (max_values_type1 - min_values_type1 + 1e-6)\n",
    "        normalized_inputs = normalized_inputs.transpose(1, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_CNN_LSTM_type1(normalized_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # validation Phase\n",
    "    model_CNN_LSTM_type1.eval()  \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            normalized_inputs = (inputs - min_values_type1) / (max_values_type1 - min_values_type1 + 1e-6)\n",
    "            normalized_inputs = normalized_inputs.transpose(1, 2)\n",
    "            outputs = model_CNN_LSTM_type1(normalized_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time() \n",
    "    epoch_duration = epoch_end_time - epoch_start_time \n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # check if this is the best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = copy.deepcopy(model_CNN_LSTM_type1.state_dict())\n",
    "        # save the best model to disk\n",
    "        torch.save(best_model_state, 'Type1CNNLSTMModel.pth')\n",
    "        print(f'Saving Best Model with loss: {avg_val_loss}')\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Epoch Duration: {epoch_duration:.2f} seconds')\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "model_CNN_LSTM_type1 = MultivariateCNNLSTM(3, target_size_type1, 18).to(device)\n",
    "model_CNN_LSTM_type1.load_state_dict(torch.load('Type1CNNLSTMModel.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T14:56:35.780651900Z",
     "start_time": "2024-02-17T14:40:41.974484Z"
    }
   },
   "id": "5a181d27c8d95b2f",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model with loss: 1.2136844799183963\n",
      "--------------\n",
      "Epoch [1/20], Epoch Duration: 112.24 seconds\n",
      "Epoch [1/20], Training Loss: 1.5526, Training Accuracy: 41.11%\n",
      "Epoch [1/20], Validation Loss: 1.2137, Validation Accuracy: 50.98%\n",
      "Saving Best Model with loss: 0.8389930594385716\n",
      "--------------\n",
      "Epoch [2/20], Epoch Duration: 118.09 seconds\n",
      "Epoch [2/20], Training Loss: 0.9935, Training Accuracy: 61.19%\n",
      "Epoch [2/20], Validation Loss: 0.8390, Validation Accuracy: 68.15%\n",
      "Saving Best Model with loss: 0.6526967760240823\n",
      "--------------\n",
      "Epoch [3/20], Epoch Duration: 119.47 seconds\n",
      "Epoch [3/20], Training Loss: 0.6953, Training Accuracy: 73.06%\n",
      "Epoch [3/20], Validation Loss: 0.6527, Validation Accuracy: 73.79%\n",
      "Saving Best Model with loss: 0.5871730472957879\n",
      "--------------\n",
      "Epoch [4/20], Epoch Duration: 118.85 seconds\n",
      "Epoch [4/20], Training Loss: 0.4761, Training Accuracy: 81.82%\n",
      "Epoch [4/20], Validation Loss: 0.5872, Validation Accuracy: 78.92%\n",
      "Saving Best Model with loss: 0.552256779973967\n",
      "--------------\n",
      "Epoch [5/20], Epoch Duration: 117.17 seconds\n",
      "Epoch [5/20], Training Loss: 0.3189, Training Accuracy: 87.80%\n",
      "Epoch [5/20], Validation Loss: 0.5523, Validation Accuracy: 81.32%\n",
      "--------------\n",
      "Epoch [6/20], Epoch Duration: 118.14 seconds\n",
      "Epoch [6/20], Training Loss: 0.2370, Training Accuracy: 91.29%\n",
      "Epoch [6/20], Validation Loss: 0.5602, Validation Accuracy: 84.58%\n",
      "Saving Best Model with loss: 0.47952881154783983\n",
      "--------------\n",
      "Epoch [7/20], Epoch Duration: 119.34 seconds\n",
      "Epoch [7/20], Training Loss: 0.1566, Training Accuracy: 94.30%\n",
      "Epoch [7/20], Validation Loss: 0.4795, Validation Accuracy: 88.92%\n",
      "--------------\n",
      "Epoch [8/20], Epoch Duration: 118.94 seconds\n",
      "Epoch [8/20], Training Loss: 0.1249, Training Accuracy: 95.45%\n",
      "Epoch [8/20], Validation Loss: 0.4919, Validation Accuracy: 88.35%\n",
      "Saving Best Model with loss: 0.4495003716856764\n",
      "--------------\n",
      "Epoch [9/20], Epoch Duration: 118.44 seconds\n",
      "Epoch [9/20], Training Loss: 0.1160, Training Accuracy: 95.84%\n",
      "Epoch [9/20], Validation Loss: 0.4495, Validation Accuracy: 90.34%\n",
      "--------------\n",
      "Epoch [10/20], Epoch Duration: 118.36 seconds\n",
      "Epoch [10/20], Training Loss: 0.1209, Training Accuracy: 95.84%\n",
      "Epoch [10/20], Validation Loss: 0.5120, Validation Accuracy: 89.60%\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "--------------\n",
      "Epoch [11/20], Epoch Duration: 119.22 seconds\n",
      "Epoch [11/20], Training Loss: 0.0692, Training Accuracy: 97.59%\n",
      "Epoch [11/20], Validation Loss: 0.5291, Validation Accuracy: 90.59%\n",
      "Saving Best Model with loss: 0.4481887238655697\n",
      "--------------\n",
      "Epoch [12/20], Epoch Duration: 117.92 seconds\n",
      "Epoch [12/20], Training Loss: 0.0236, Training Accuracy: 99.29%\n",
      "Epoch [12/20], Validation Loss: 0.4482, Validation Accuracy: 92.82%\n",
      "--------------\n",
      "Epoch [13/20], Epoch Duration: 118.00 seconds\n",
      "Epoch [13/20], Training Loss: 0.0163, Training Accuracy: 99.52%\n",
      "Epoch [13/20], Validation Loss: 0.4620, Validation Accuracy: 92.94%\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "--------------\n",
      "Epoch [14/20], Epoch Duration: 117.85 seconds\n",
      "Epoch [14/20], Training Loss: 0.0135, Training Accuracy: 99.64%\n",
      "Epoch [14/20], Validation Loss: 0.4634, Validation Accuracy: 93.37%\n",
      "--------------\n",
      "Epoch [15/20], Epoch Duration: 118.74 seconds\n",
      "Epoch [15/20], Training Loss: 0.0108, Training Accuracy: 99.74%\n",
      "Epoch [15/20], Validation Loss: 0.4677, Validation Accuracy: 93.40%\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n",
      "--------------\n",
      "Epoch [16/20], Epoch Duration: 119.21 seconds\n",
      "Epoch [16/20], Training Loss: 0.0104, Training Accuracy: 99.75%\n",
      "Epoch [16/20], Validation Loss: 0.4696, Validation Accuracy: 93.38%\n",
      "--------------\n",
      "Epoch [17/20], Epoch Duration: 119.11 seconds\n",
      "Epoch [17/20], Training Loss: 0.0100, Training Accuracy: 99.78%\n",
      "Epoch [17/20], Validation Loss: 0.4696, Validation Accuracy: 93.41%\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n",
      "--------------\n",
      "Epoch [18/20], Epoch Duration: 119.55 seconds\n",
      "Epoch [18/20], Training Loss: 0.0100, Training Accuracy: 99.77%\n",
      "Epoch [18/20], Validation Loss: 0.4695, Validation Accuracy: 93.42%\n",
      "--------------\n",
      "Epoch [19/20], Epoch Duration: 119.55 seconds\n",
      "Epoch [19/20], Training Loss: 0.0099, Training Accuracy: 99.79%\n",
      "Epoch [19/20], Validation Loss: 0.4695, Validation Accuracy: 93.42%\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "--------------\n",
      "Epoch [20/20], Epoch Duration: 118.51 seconds\n",
      "Epoch [20/20], Training Loss: 0.0099, Training Accuracy: 99.79%\n",
      "Epoch [20/20], Validation Loss: 0.4696, Validation Accuracy: 93.42%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train second model\n",
    "target_size_type2 = 1169\n",
    "whole_dataset_type2 = TrainDataframeWithLabels(data_type_2, '2', target_size_type2)\n",
    "model_CNN_LSTM_type2 = MultivariateCNNLSTM(3, target_size_type2, 18).to(device)\n",
    "\n",
    "labels = data_type_2['activity'].to_list()\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(whole_dataset_type2)),\n",
    "    test_size=0.2,  \n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "# create stratified datasets\n",
    "train_dataset = Subset(whole_dataset_type2, train_idx)\n",
    "val_dataset = Subset(whole_dataset_type2, val_idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN_LSTM_type2.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "\n",
    "best_val_loss = float('inf') \n",
    "best_model_state = copy.deepcopy(model_CNN_LSTM_type2.state_dict()) \n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    total_val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    # training Phase\n",
    "    model_CNN_LSTM_type2.train()  \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        normalized_inputs = (inputs - min_values_type2) / (max_values_type2 - min_values_type2 + 1e-6)\n",
    "        normalized_inputs = normalized_inputs.transpose(1, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_CNN_LSTM_type2(normalized_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # validation Phase\n",
    "    model_CNN_LSTM_type2.eval() \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            normalized_inputs = (inputs - min_values_type2) / (max_values_type2 - min_values_type2 + 1e-6)\n",
    "            normalized_inputs = normalized_inputs.transpose(1, 2)\n",
    "            outputs = model_CNN_LSTM_type2(normalized_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = copy.deepcopy(model_CNN_LSTM_type2.state_dict())\n",
    "        torch.save(best_model_state, 'Type2CNNLSTMModel.pth')\n",
    "        print(f'Saving Best Model with loss: {avg_val_loss}')\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Epoch Duration: {epoch_duration:.2f} seconds')\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "model_CNN_LSTM_type2 = MultivariateCNNLSTM(3, target_size_type2, 18).to(device)\n",
    "model_CNN_LSTM_type2.load_state_dict(torch.load('Type2CNNLSTMModel.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:08:26.411883400Z",
     "start_time": "2024-02-17T16:28:57.517479500Z"
    }
   },
   "id": "83443c5136363db",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# results of feature extractor without LSTM layer were better, so we did not proceed with this code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97cdcde962fbf10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
