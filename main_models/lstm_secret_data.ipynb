{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-25T12:54:41.463501700Z",
     "start_time": "2024-02-25T12:54:38.665724600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from models_utils.GLOBALS import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get train data\n",
    "data = pd.read_csv('../csv/secret_all_data_features.csv')\n",
    "data['sensor'] = data['sensor'].map(sensor_mapping)\n",
    "data.drop('sample_id', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T12:54:48.933595800Z",
     "start_time": "2024-02-25T12:54:48.441179300Z"
    }
   },
   "id": "449c011aca182973",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# LSTM model to continue the sequence\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step\n",
    "        return out\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, features, labels=None, seq_length=10):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_end = idx + self.seq_length\n",
    "        sequence_features = self.features.iloc[idx:idx_end].values\n",
    "        sequence_features = torch.tensor(sequence_features, dtype=torch.float)\n",
    "        if self.labels is not None:\n",
    "            label = self.labels.iloc[idx_end] if self.labels is not None else None\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return sequence_features, label\n",
    "        return sequence_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T12:54:51.947612700Z",
     "start_time": "2024-02-25T12:54:51.945106Z"
    }
   },
   "id": "66ad565896bf514a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# prepare data\n",
    "sequence_length = 500\n",
    "train_data = data[:77304].copy()\n",
    "train_data['activity'] = train_data['activity'].map(activity_id_mapping)\n",
    "features_columns = data.columns.drop('activity').tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T12:54:56.751035500Z",
     "start_time": "2024-02-25T12:54:56.726328100Z"
    }
   },
   "id": "9e8b82898eddacb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = pd.DataFrame(scaler.fit_transform(train_data.drop('activity', axis=1)),\n",
    "                                     columns=train_data.columns[:-1])\n",
    "train_data_normalized['activity'] = train_data['activity']\n",
    "train_data = train_data_normalized"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T12:54:59.108932300Z",
     "start_time": "2024-02-25T12:54:59.067492100Z"
    }
   },
   "id": "292f627fc74b1778",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Epoch 1, Train Loss: 2.3322, Val Loss: 1.9029\n",
      "Saved new best model at epoch 1 with validation loss: 1.9029\n",
      "---------------------------------------\n",
      "Epoch 2, Train Loss: 1.9763, Val Loss: 1.4168\n",
      "Saved new best model at epoch 2 with validation loss: 1.4168\n",
      "---------------------------------------\n",
      "Epoch 3, Train Loss: 1.5397, Val Loss: 1.6181\n",
      "---------------------------------------\n",
      "Epoch 4, Train Loss: 0.8439, Val Loss: 1.1371\n",
      "Saved new best model at epoch 4 with validation loss: 1.1371\n",
      "---------------------------------------\n",
      "Epoch 5, Train Loss: 0.5264, Val Loss: 0.5038\n",
      "Saved new best model at epoch 5 with validation loss: 0.5038\n",
      "---------------------------------------\n",
      "Epoch 6, Train Loss: 0.4854, Val Loss: 0.3849\n",
      "Saved new best model at epoch 6 with validation loss: 0.3849\n",
      "---------------------------------------\n",
      "Epoch 7, Train Loss: 0.3322, Val Loss: 0.3543\n",
      "Saved new best model at epoch 7 with validation loss: 0.3543\n",
      "---------------------------------------\n",
      "Epoch 8, Train Loss: 0.2269, Val Loss: 0.1871\n",
      "Saved new best model at epoch 8 with validation loss: 0.1871\n",
      "---------------------------------------\n",
      "Epoch 9, Train Loss: 0.1272, Val Loss: 0.0885\n",
      "Saved new best model at epoch 9 with validation loss: 0.0885\n",
      "---------------------------------------\n",
      "Epoch 10, Train Loss: 0.1453, Val Loss: 0.1361\n",
      "---------------------------------------\n",
      "Epoch 11, Train Loss: 0.1141, Val Loss: 0.0999\n",
      "---------------------------------------\n",
      "Epoch 12, Train Loss: 0.0735, Val Loss: 0.0568\n",
      "Saved new best model at epoch 12 with validation loss: 0.0568\n",
      "---------------------------------------\n",
      "Epoch 13, Train Loss: 0.0595, Val Loss: 0.0487\n",
      "Saved new best model at epoch 13 with validation loss: 0.0487\n",
      "---------------------------------------\n",
      "Epoch 14, Train Loss: 0.0749, Val Loss: 0.0311\n",
      "Saved new best model at epoch 14 with validation loss: 0.0311\n",
      "---------------------------------------\n",
      "Epoch 15, Train Loss: 0.0732, Val Loss: 0.0288\n",
      "Saved new best model at epoch 15 with validation loss: 0.0288\n",
      "---------------------------------------\n",
      "Epoch 16, Train Loss: 0.0306, Val Loss: 0.0212\n",
      "Saved new best model at epoch 16 with validation loss: 0.0212\n",
      "---------------------------------------\n",
      "Epoch 17, Train Loss: 0.0462, Val Loss: 0.0244\n",
      "---------------------------------------\n",
      "Epoch 18, Train Loss: 0.0194, Val Loss: 0.0178\n",
      "Saved new best model at epoch 18 with validation loss: 0.0178\n",
      "---------------------------------------\n",
      "Epoch 19, Train Loss: 0.1411, Val Loss: 0.0902\n",
      "---------------------------------------\n",
      "Epoch 20, Train Loss: 0.0360, Val Loss: 0.0191\n",
      "---------------------------------------\n",
      "Epoch 00021: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 21, Train Loss: 0.0257, Val Loss: 0.0203\n",
      "---------------------------------------\n",
      "Epoch 22, Train Loss: 0.0140, Val Loss: 0.0165\n",
      "Saved new best model at epoch 22 with validation loss: 0.0165\n",
      "---------------------------------------\n",
      "Epoch 23, Train Loss: 0.0258, Val Loss: 0.0162\n",
      "Saved new best model at epoch 23 with validation loss: 0.0162\n",
      "---------------------------------------\n",
      "Epoch 24, Train Loss: 0.0189, Val Loss: 0.0141\n",
      "Saved new best model at epoch 24 with validation loss: 0.0141\n",
      "---------------------------------------\n",
      "Epoch 25, Train Loss: 0.0195, Val Loss: 0.0288\n",
      "---------------------------------------\n",
      "Epoch 26, Train Loss: 0.0325, Val Loss: 0.0140\n",
      "Saved new best model at epoch 26 with validation loss: 0.0140\n",
      "---------------------------------------\n",
      "Epoch 27, Train Loss: 0.0157, Val Loss: 0.0228\n",
      "---------------------------------------\n",
      "Epoch 28, Train Loss: 0.0192, Val Loss: 0.0126\n",
      "Saved new best model at epoch 28 with validation loss: 0.0126\n",
      "---------------------------------------\n",
      "Epoch 29, Train Loss: 0.0397, Val Loss: 0.0130\n",
      "---------------------------------------\n",
      "Epoch 30, Train Loss: 0.0183, Val Loss: 0.0216\n",
      "---------------------------------------\n",
      "Epoch 00031: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 31, Train Loss: 0.0144, Val Loss: 0.0481\n",
      "---------------------------------------\n",
      "Epoch 32, Train Loss: 0.0117, Val Loss: 0.0112\n",
      "Saved new best model at epoch 32 with validation loss: 0.0112\n",
      "---------------------------------------\n",
      "Epoch 33, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Saved new best model at epoch 33 with validation loss: 0.0104\n",
      "---------------------------------------\n",
      "Epoch 34, Train Loss: 0.0106, Val Loss: 0.0105\n",
      "---------------------------------------\n",
      "Epoch 35, Train Loss: 0.0171, Val Loss: 0.0190\n",
      "---------------------------------------\n",
      "Epoch 36, Train Loss: 0.0110, Val Loss: 0.0100\n",
      "Saved new best model at epoch 36 with validation loss: 0.0100\n",
      "---------------------------------------\n",
      "Epoch 37, Train Loss: 0.0105, Val Loss: 0.0101\n",
      "---------------------------------------\n",
      "Epoch 38, Train Loss: 0.0092, Val Loss: 0.0092\n",
      "Saved new best model at epoch 38 with validation loss: 0.0092\n",
      "---------------------------------------\n",
      "Epoch 39, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Saved new best model at epoch 39 with validation loss: 0.0090\n",
      "---------------------------------------\n",
      "Epoch 40, Train Loss: 0.0076, Val Loss: 0.0075\n",
      "Saved new best model at epoch 40 with validation loss: 0.0075\n",
      "---------------------------------------\n",
      "Epoch 41, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "---------------------------------------\n",
      "Epoch 42, Train Loss: 0.0092, Val Loss: 0.0079\n",
      "---------------------------------------\n",
      "Epoch 00043: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 43, Train Loss: 0.0111, Val Loss: 0.0099\n",
      "---------------------------------------\n",
      "Epoch 44, Train Loss: 0.0133, Val Loss: 0.0097\n",
      "---------------------------------------\n",
      "Epoch 45, Train Loss: 0.0076, Val Loss: 0.0082\n",
      "---------------------------------------\n",
      "Epoch 00046: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 46, Train Loss: 0.0069, Val Loss: 0.0083\n",
      "---------------------------------------\n",
      "Epoch 47, Train Loss: 0.0065, Val Loss: 0.0074\n",
      "Saved new best model at epoch 47 with validation loss: 0.0074\n",
      "---------------------------------------\n",
      "Epoch 48, Train Loss: 0.0062, Val Loss: 0.0120\n",
      "---------------------------------------\n",
      "Epoch 49, Train Loss: 0.0063, Val Loss: 0.0071\n",
      "Saved new best model at epoch 49 with validation loss: 0.0071\n",
      "---------------------------------------\n",
      "Epoch 50, Train Loss: 0.0056, Val Loss: 0.0080\n",
      "---------------------------------------\n",
      "Epoch 51, Train Loss: 0.0067, Val Loss: 0.0076\n",
      "---------------------------------------\n",
      "Epoch 52, Train Loss: 0.0058, Val Loss: 0.0070\n",
      "Saved new best model at epoch 52 with validation loss: 0.0070\n",
      "---------------------------------------\n",
      "Epoch 53, Train Loss: 0.0057, Val Loss: 0.0073\n",
      "---------------------------------------\n",
      "Epoch 54, Train Loss: 0.0075, Val Loss: 0.0068\n",
      "Saved new best model at epoch 54 with validation loss: 0.0068\n",
      "---------------------------------------\n",
      "Epoch 55, Train Loss: 0.0056, Val Loss: 0.0079\n",
      "---------------------------------------\n",
      "Epoch 56, Train Loss: 0.0058, Val Loss: 0.0089\n",
      "---------------------------------------\n",
      "Epoch 00057: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 57, Train Loss: 0.0095, Val Loss: 0.0097\n",
      "---------------------------------------\n",
      "Epoch 58, Train Loss: 0.0053, Val Loss: 0.0076\n",
      "---------------------------------------\n",
      "Epoch 59, Train Loss: 0.0046, Val Loss: 0.0059\n",
      "Saved new best model at epoch 59 with validation loss: 0.0059\n",
      "---------------------------------------\n",
      "Epoch 60, Train Loss: 0.0047, Val Loss: 0.0068\n",
      "---------------------------------------\n",
      "Epoch 61, Train Loss: 0.0046, Val Loss: 0.0070\n",
      "---------------------------------------\n",
      "Epoch 00062: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 62, Train Loss: 0.0048, Val Loss: 0.0066\n",
      "---------------------------------------\n",
      "Epoch 63, Train Loss: 0.0042, Val Loss: 0.0069\n",
      "---------------------------------------\n",
      "Epoch 64, Train Loss: 0.0049, Val Loss: 0.0075\n",
      "---------------------------------------\n",
      "Epoch 00065: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 65, Train Loss: 0.0039, Val Loss: 0.0103\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 41\u001B[0m\n\u001B[0;32m     39\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     40\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 41\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     43\u001B[0m avg_train_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# validation \u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train LSTM model\n",
    "train_dataset = SequenceDataset(train_data[features_columns], train_data['activity'], sequence_length)\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = total_size - train_size  \n",
    "\n",
    "\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True) \n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "best_loss = float('inf') \n",
    "best_model_path = '../models/best_model_LSTM_features_cnn5.pth' \n",
    "\n",
    "# model\n",
    "input_dim = len(features_columns)  \n",
    "hidden_dim = 128  \n",
    "layer_dim = 2  \n",
    "output_dim = len(train_data['activity'].unique())  \n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n",
    "\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    print('---------------------------------------')\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # validation \n",
    "    model.eval()  \n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:  \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # save the best model\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Saved new best model at epoch {epoch + 1} with validation loss: {best_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:08:34.015727300Z",
     "start_time": "2024-02-25T12:57:16.575506400Z"
    }
   },
   "id": "e1dcf431cacd3009",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model = model.to(device)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:08:35.864153100Z",
     "start_time": "2024-02-25T13:08:35.856918Z"
    }
   },
   "id": "f18b54a05b009d69",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get test data\n",
    "test_data = data[77304-sequence_length:].copy()\n",
    "test_data = pd.DataFrame(scaler.transform(test_data.drop('activity', axis=1)), columns=test_data.columns[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:21:40.591444Z",
     "start_time": "2024-02-25T13:21:40.560977400Z"
    }
   },
   "id": "72e06b54a0fb178f",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get and save results for test data\n",
    "test_dataset = SequenceDataset(test_data[features_columns], seq_length=sequence_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs.to(device)  # Move to device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
    "predictions_df['activity'] = predictions_df['prediction'].map(id_activity_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:21:44.955581900Z",
     "start_time": "2024-02-25T13:21:41.105325400Z"
    }
   },
   "id": "b3e7116013b46ed7",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = pd.read_csv('../csv/secret_all_data_features.csv')[77304:]['sample_id'].rename({'sample_id':'id'}).reset_index(drop=True)\n",
    "results['activity'] = predictions_df['activity']\n",
    "results.to_csv('../csv/lstm_results5.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T13:26:24.830478500Z",
     "start_time": "2024-02-25T13:26:24.340691700Z"
    }
   },
   "id": "91542315e5c82f9c",
   "execution_count": 97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
